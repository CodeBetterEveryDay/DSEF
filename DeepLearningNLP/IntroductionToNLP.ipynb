{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction to Natural Language Processing with Python 29th July 2017**\n",
    "\n",
    "Agenda:\n",
    "\n",
    "1.\tIntroduction to Natural Language Processing \n",
    "2.\tTextblob: Simplified Text Processing\n",
    "3.\tScikit Learn (text_analytics)\n",
    "4.\tNLTK - Natural Language Toolkit\n",
    "5.\tSpaCy    (advanced NLP techniques) \n",
    "6.\tGensim   (topic modelling)\n",
    "7.\tFuture work –  Project.  Development of a predictive web application. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installation:**\n",
    "\n",
    "The recommended installation is the Anaconda distribution with Python 3.\n",
    "https://www.continuum.io/downloads\n",
    "\n",
    "You can install relevant Python NLP Libraries with conda.\n",
    "- conda install -c anaconda nltk\n",
    "- conda install -c conda-forge textblob \n",
    "- conda install -c conda-forge spacy\n",
    "- conda install -c anaconda gensim\n",
    "\n",
    "**Data**\n",
    "http://www.nltk.org/data.html \n",
    "\n",
    "Kaggle links with text data competitions:\n",
    "\n",
    "https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**1 Introduction to Natural Language Processing**\n",
    "\n",
    "Natural language processing is a complex field and is the intersection of computer science, artificial intelligence and computational linguistics.   https://en.wikipedia.org/wiki/Natural_language_processing\n",
    "\n",
    "Natural Language Processing (NLP) is ability of machines to understand and interpret human language the way it is written or spoken.\n",
    "\n",
    "We will start with written language – text data.Roughly 80% of the world data is unstructured. Majority of this data is in text worm.\n",
    "\n",
    "A computational challenge for NLP exists because human language is ambiguous, needs context and ability to link concepts.\n",
    "\n",
    "The ambiguity present in natural language:\n",
    "- Lexical Ambiguity — Words have multiple meanings\n",
    "- Syntactic Ambiguity — Sentence having multiple parse trees.\n",
    "- Semantic Ambiguity — Sentence having multiple meanings\n",
    "- Anaphoric Ambiguity — Phrase or word which is previously mentioned but has a different meaning.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Natural language processing types:    \n",
    "-\tText classification - Spam detection    \n",
    "-\tSentiment analysis   \n",
    "-\tMachine translation \n",
    "-\tSummarising blocks of text \n",
    "-\tNamed entity recognition \n",
    "-\tAutomatic speech recognition Siri, Alexa or Google Now\n",
    "-\tChatbots   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Natural language processing  terms:**\n",
    "    \n",
    "A **corpus** is a collection of text in digital form (digital documents) assembled for text processing.\n",
    "\n",
    "It is also called a training corpus. This inferred latent structure can be later used to assign topics to new documents, which did not appear in the training corpus.\n",
    "\n",
    "A **token** is a single chopped element of the sentence (line) which can be a word or a mix of word, characters or punctuation signs. The list of tokens becomes input for further processing such as parsing or text mining.\n",
    "\n",
    "This process of chopping our text documents up into pieces or chunks is called tokenization.\n",
    "\n",
    "**N-Grams** is continuous sequence of n items from a given sequence of text or speech.\n",
    "\n",
    "Unigram, bigram, n-gram: sequence of 1,2 or n words taken as the basic element\n",
    "\n",
    "**Stopwords** are very common words that are removed as not meaningful ( have no intrinsic meaning). For example:       a, an, the, is, which.\n",
    "\n",
    "In **Bag-of-words** is a simple model which discard sentence structure. In Bag-of-words unordered collection of words. \n",
    "\n",
    "Alternative to **Bag-of-words**\n",
    "\n",
    "**Feature hashing** or the **hashing trick**\n",
    "https://en.wikipedia.org/wiki/Feature_hashing\n",
    "https://www.quora.com/Can-you-explain-feature-hashing-in-an-easily-understandable-way\n",
    "\n",
    "**Rescaling** or **Normalisation** the data with tf-idf.\n",
    "**Tf–idf** for “Term Frequency times Inverse Document Frequency” is a way to represent documents as feature vectors.\n",
    "\n",
    "The tf-idf model transforms vectors from the bag-of-words representation to a vector space where the frequency counts are weighted according to the relative rarity of each word in the corpus. It ss not linear transformation.\n",
    " https://en.wikipedia.org/wiki/Tf–idf\n",
    "    \n",
    "**Stemming** is the process of finding the root of the word. A direct approach that chops off the ending of the word to limit variation For example, \"go\", \"goes\", \"going\"  will be \"go\".Different versions of stemmers in NLTK: porter, snowball and wordnetlemmatizer.\n",
    "\n",
    "\n",
    "**Lemmatization** is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. For example, \"sing\", \"sang\" and \"singing\" are all different \"forms\" of the lemma sing. The context of the sentence is also preserved in lemmatization as opposed to stemming. \n",
    "\n",
    "\n",
    "We still need to remove the non-words like punctuation marks or special characters from the documents.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "POS-tagger, or a part-of-speech tagger, processes a sequence of words, and attaches a part of speech tag to each word. For example, given a text, assigns roles to each word: noun, verb, adjective, pronoun, adverb, article, conjunction, preposition and interjection.\n",
    "\n",
    "Syntatic dependency describe how eeach type of word relats to each other in the sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment Analysis** – The use of Natural Language Processing techniques to extract subjective information from a piece of text. i.e. whether an author is being subjective or objective or even positive or negative\n",
    "\n",
    "**Named Entity Recognition (NER)** – The process of locating and classifying elements in text into predefined categories such as the names of people, organizations, places, monetary values, percentages, etc.\n",
    "\n",
    "**Latent Semantic Analysis (LSA)** – The process of analyzing relationships between a set of documents and the terms they contain. Accomplished by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text.\n",
    "\n",
    "**Latent Dirichlet Allocation (LDA)** – A common topic modeling technique, LDA is based on the premise that each document or piece of text is a mixture of a small number of topics and that each word in a document is attributable to one of the topics. http://www.datasciencecentral.com/profiles/blogs/10-common-nlp-terms-explained-for-the-text-analysis-novice\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Latent Semantic Indexing, LSI (or sometimes LSA) transforms documents from either bag-of-words or (preferrably) TfIdf-weighted space into a latent space of a lower dimensionality\n",
    "\n",
    "Latent Dirichlet Allocation, LDA is yet another transformation from bag-of-words counts into a topic space of lower dimensionality. LDA is a probabilistic extension of LSA (also called multinomial PCA), so LDA’s topics can be interpreted as probability distributions over words. These distributions are, just like with LSA, inferred automatically from a training corpus. Documents are in turn interpreted as a (soft) mixture of these topics (again, just like with LSA).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Vector space model\n",
    "https://en.wikipedia.org/wiki/Vector_space_model\n",
    "\n",
    "\n",
    "Word Embeddings are words converted into numbers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Word2vec** is a combination of two techniques (CBOW(Continuous Bag-of-Words model) and Skip-Gram model) that are used to produce word embeddings. \n",
    "https://en.wikipedia.org/wiki/Word2vec\n",
    "\n",
    "Word2Vec Tomas Mikolov \n",
    "http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 TextBlob: Simplified Text Processing**\n",
    "\n",
    "TextBlob is a Python library for processing textual data. It provides a simple API for diving into common Natural Language Processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more. TextBlob stands on the giant shoulders of NLTK and pattern. TextBlob is good for an initial prototyping. Language translation and detection is powered by the Google Translate API.https://cloud.google.com/translate/docs/translating-text#language-params\n",
    "\n",
    "http://textblob.readthedocs.io/en/dev/quickstart.html\n",
    "\n",
    "http://textblob.readthedocs.io/en/dev/classifiers.html#classifiers\n",
    "\n",
    "\n",
    "Very good tutorial by Allison Parrish http://rwet.decontextualize.com/book/textblob/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 Scikit Learn text_analytics** \n",
    "\n",
    " http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4 NLTK** is the Natural Language Toolkit in Python. It work with human language data and it provides over 50 datasets (corpora and lexical resources), along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning.\n",
    "\n",
    "NLTK is the most popular library for doing NLP in Python. Drawbacks it is not optimised.  http://www.nltk.org/\n",
    "\n",
    "Book **Natural Language Processing with Python** – Analyzing Text with the Natural Language Toolkit \n",
    "\n",
    "by Steven Bird, Ewan Klein, and Edward Loper  http://www.nltk.org/book/  http://www.nltk.org/book_1ed/\n",
    "\n",
    "\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5 Spacy** is Python library for advanced Natural Language Processing, written in the programming languages Python and Cython. It offers the fastest syntactic parser in the world. https://spacy.io/\n",
    "\n",
    "It is currently supports English and German, as well as tokenization for Chinese and several other languages.\n",
    "\n",
    "https://www.quora.com/What-are-the-advantages-of-Spacy-vs-NLTK\n",
    "\n",
    "\n",
    "Installation https://spacy.io/docs/usage/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6 Gensim** is a  Python library designed to process raw, unstructured texts and extract semantic topics from documents. It is well optimised. The algorithms in gensim, such as Latent Semantic Analysis, Latent Dirichlet Allocation and Random Projections discover semantic structure of documents by examining statistical co-occurrence patterns of the words within a corpus of training documents. These algorithms are unsupervised.\n",
    "\n",
    "https://github.com/RaRe-Technologies/gensim\n",
    "\n",
    "https://radimrehurek.com/gensim/intro.html\n",
    "\n",
    "https://radimrehurek.com/gensim/tutorial.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Experiments on the English Wikipedia. It can take some time.\n",
    "https://radimrehurek.com/gensim/wiki.html\n",
    "\n",
    "\n",
    "https://dumps.wikimedia.org/enwiki/20170401/\n",
    "\n",
    "https://radimrehurek.com/gensim/distributed.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical exercises:**\n",
    "\n",
    "Exercise 1 Explore TextBlob http://textblob.readthedocs.io/en/dev/quickstart.html\n",
    "http://textblob.readthedocs.io/en/dev/classifiers.html#classifiers\n",
    "\n",
    "Exercise 2: Scikit Learn text_analytics of the 20 Newsgroups data from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "\n",
    "Exercise 3: Sentiment Analysis on movie reviews using \n",
    "\n",
    "- Scikit Learn \n",
    "\n",
    "Write a text classification pipeline to classify movie reviews as either positive or negative.\n",
    "Find a good set of parameters using grid search.\n",
    "Evaluate the performance on a held out test set.\n",
    "\n",
    "- NLTK\n",
    "\n",
    "- Try to do Sentiment Analysis on movie reviews with data from https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data\n",
    "\n",
    "- Advance: use SpaCy and Gensim https://www.kaggle.com/c/word2vec-nlp-tutorial/data   (part-2-word-vectors has errors needs to be fixed)\n",
    "\n",
    "- Present results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
